# Neural-Network-From-Scratch-with-Automatic-Differentiation-Python
This notebook demonstrates how a neural network works internally, built completely from scratch using Python, without relying on high-level deep learning frameworks for the core logic.
The project starts with basic mathematical functions and derivatives, gradually building up to:

Automatic differentiation (autograd)

Computational graphs

Backpropagation

A multi-layer perceptron (MLP)

Training a neural network using gradient descent

The goal is to deeply understand how modern deep learning frameworks (like PyTorch) work under the hood.
